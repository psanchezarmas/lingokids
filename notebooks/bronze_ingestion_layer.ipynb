{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ad40e44-d999-4f06-940c-6f55625b3bb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# src/ingestion/bronze_ingest.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (BooleanType, DoubleType, IntegerType,\n",
    "                               StringType, StructField, StructType, ArrayType,\n",
    "                               StructType,TimestampType)\n",
    "\n",
    "# Mapping YAML/JSON types to Spark types\n",
    "TYPE_MAP = {\n",
    "    \"string\": StringType(),\n",
    "    \"int\": IntegerType(),\n",
    "    \"integer\": IntegerType(),\n",
    "    \"double\": DoubleType(),\n",
    "    \"timestamp\": TimestampType(),\n",
    "    \"boolean\": BooleanType(),\n",
    "    \"struct\": StructType(), \n",
    "    \"array\": ArrayType(StringType()), \n",
    "}\n",
    "\n",
    "def build_spark_schema(columns: list) -> StructType:\n",
    "    \"\"\"\n",
    "    Build a Spark StructType schema from a list of columns from YAML config.\n",
    "    Supports nested structs and arrays.\n",
    "    \"\"\"\n",
    "    fields = []\n",
    "    for col in columns:\n",
    "        col_type = col[\"type\"].lower()\n",
    "        nullable = col.get(\"nullable\", True)\n",
    "\n",
    "        if col_type == \"struct\":\n",
    "            # recursively build schema for nested struct\n",
    "            struct_fields = build_spark_schema(col[\"fields\"])\n",
    "            fields.append(StructField(col[\"name\"], struct_fields, nullable))\n",
    "        elif col_type == \"array\":\n",
    "            # assume array of strings by default, but can also be nested structs\n",
    "            element_type = col.get(\"element_type\", \"string\").lower()\n",
    "            if element_type == \"struct\":\n",
    "                element_schema = build_spark_schema(col[\"fields\"])\n",
    "                fields.append(StructField(col[\"name\"], ArrayType(element_schema), nullable))\n",
    "            else:\n",
    "                fields.append(StructField(col[\"name\"], TYPE_MAP[element_type], nullable))\n",
    "        else:\n",
    "            fields.append(StructField(col[\"name\"], TYPE_MAP[col_type], nullable))\n",
    "\n",
    "    return StructType(fields)\n",
    "\n",
    "def validate_yaml_against_json_schema(columns: list, schema_file: str, yaml_file: str = None):\n",
    "    \"\"\"\n",
    "    Validate that all fields in the JSON schema exist in the YAML columns.\n",
    "    Raises ValueError if any required fields are missing.\n",
    "    \"\"\"\n",
    "    with open(schema_file, \"r\") as f:\n",
    "        schema_json = json.load(f)\n",
    "\n",
    "    yaml_fields = {col[\"name\"] for col in columns}\n",
    "    json_fields = {field[\"name\"] for field in schema_json[\"fields\"]}\n",
    "    missing_fields = json_fields - yaml_fields\n",
    "    if missing_fields:\n",
    "        raise ValueError(f\"{yaml_file or 'YAML config'} is missing columns defined in JSON schema: {missing_fields}\")\n",
    "\n",
    "def ingest_json_to_bronze(\n",
    "    json_file: str,\n",
    "    yaml_file: str,\n",
    "    ingestion_type: str = \"overwrite\"\n",
    "):\n",
    "    \"\"\"| `ingestion_type` / `mode`      | Description                                                         | Typical Use Case                                   |\n",
    "        | ------------------------------ | ------------------------------------------------------------------- | -------------------------------------------------- |\n",
    "        | `\"overwrite\"`                  | Replaces the existing table with the new data.                      | Full refresh of a table.                           |\n",
    "        | `\"append\"`                     | Adds new rows to the existing table without removing existing data. | Incremental ingestion / new batch of data.         |\n",
    "        | `\"ignore\"`                     | If the table already exists, Spark does nothing.                    | Avoid overwriting existing tables.                 |\n",
    "        | `\"error\"` or `\"errorifexists\"` | Raises an error if the table already exists.                        | Strict ingestion to prevent accidental overwrites. |\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Ingest a single JSON file to bronze Delta table using its YAML config.\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Load YAML\n",
    "    with open(yaml_file, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    table_name = config[\"table_name\"]\n",
    "    schema_file = config.get(\"schema_file\")\n",
    "    columns = config[\"columns\"]\n",
    "\n",
    "    # Validate if schema_file is provided\n",
    "    if schema_file:\n",
    "        validate_yaml_against_json_schema(columns, schema_file, yaml_file)\n",
    "\n",
    "    # Build Spark schema\n",
    "    spark_schema = build_spark_schema(columns)\n",
    "\n",
    "    # Read JSON\n",
    "    df = spark.read.schema(spark_schema).json(json_file)\n",
    "\n",
    "    # Write to catalog as Delta table Bronze\n",
    "    catalog_table = f\"lingokids.bronze.{table_name}\"\n",
    "    df.write.format(\"delta\").mode(ingestion_type).saveAsTable(catalog_table)\n",
    "\n",
    "    return (f\"lingokids.bronze.{table_name} ingested from {json_file}\", catalog_table)\n",
    "\n",
    "def ingest_all_yaml_configs(\n",
    "    yaml_dir: str = \"configs/bronze\",\n",
    "    ingestion_type: str = \"overwrite\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Scan all YAML configs in yaml_dir and ingest their corresponding JSON files.\n",
    "    Returns a dictionary of table_name -> target_path.\n",
    "    \"\"\"\n",
    "    yaml_files = glob(os.path.join(yaml_dir, \"*.yaml\"))\n",
    "    results = {}\n",
    "\n",
    "    for yaml_file in yaml_files:\n",
    "        with open(yaml_file, \"r\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        json_file = config[\"json_file\"]\n",
    "        table_name = config[\"table_name\"]\n",
    "        target_path = ingest_json_to_bronze(json_file, yaml_file, ingestion_type)\n",
    "        results[table_name] = target_path\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9759ec7-4480-4f60-8a7b-01b51b8631d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "yaml_dir = \"/Workspace/Users/pablo.sanchez.armas@gmail.com/lingokids/configs/bronze\"\n",
    "ingested_tables = ingest_all_yaml_configs(yaml_dir=yaml_dir)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_ingestion_layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
