{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aedfb615-5fdf-4a2e-8870-fb9463478960",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " Bronze → Silver ETL (Incremental with Merge)\n",
    "\n",
    "Loads events from Bronze to Silver using config and prevents duplicates via merge on `event_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc26cb0b-3cdc-4cae-948d-532cf7546098",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp, col, to_timestamp, date_format\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0da7a056-f950-43dc-a419-54aa5a1ab77a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config_path = \"/Workspace/Users/pablo.sanchez.armas@gmail.com/lingokids/configs/silver/events.yaml\"\n",
    "\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "source_cfg = config[\"source\"]\n",
    "target_cfg = config[\"target\"]\n",
    "load_cfg = config[\"load\"]\n",
    "\n",
    "bronze_table = f\"{source_cfg['catalog']}.{source_cfg['schema']}.{source_cfg['table']}\"\n",
    "silver_table = f\"{target_cfg['catalog']}.{target_cfg['schema']}.{target_cfg['table']}\"\n",
    "\n",
    "merge_key = load_cfg.get(\"merge_key\")\n",
    "if not merge_key:\n",
    "    raise ValueError(\"merge_key must be defined in the config file\")\n",
    "\n",
    "print(f\"Source: {bronze_table}\")\n",
    "print(f\"Target: {silver_table}\")\n",
    "print(f\"Merge Key: {merge_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddbe35f4-291f-421c-b28b-0c9e7d0211e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read from bronze\n",
    "df_bronze = spark.read.table(bronze_table)\n",
    "\n",
    "# Add processing timestamp\n",
    "df_bronze = df_bronze.withColumn(\"updated_at\", current_timestamp())\n",
    "\n",
    "# Convert occurred_at to timestamp and then to datekey (yyyymmdd)\n",
    "df_bronze = df_bronze.withColumn(\n",
    "    \"date_key\",\n",
    "    date_format(to_timestamp(\"occurred_at\", \"yyyy-MM-dd'T'HH:mm:ss.SSSXXX\"), \"yyyyMMdd\")\n",
    ")\n",
    "\n",
    "df_data = df_bronze.select(\n",
    "    \"event_id\",\n",
    "    col(\"data.*\"),\n",
    "    \"updated_at\",\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Split \"context\" into a separate DataFrame and flatten nested structs\n",
    "# -------------------------\n",
    "\n",
    "# Rename the nested fields first\n",
    "df_context_renamed = df_bronze.withColumn(\"os_version\", col(\"context.os.version\")) \\\n",
    "                               .withColumn(\"app_version\", col(\"context.app.version\"))\n",
    "\n",
    "\n",
    "# Now flatten the rest of the context struct (excluding the already renamed fields)\n",
    "df_context = df_context_renamed.select(\n",
    "    \"event_id\",\n",
    "    col(\"context.device.*\"),\n",
    "    col(\"context.os.name\").alias(\"os_name\"),\n",
    "    col(\"context.app.build\").alias(\"app_build\"),\n",
    "    \"os_version\",\n",
    "    \"app_version\",\n",
    "    \"updated_at\"\n",
    ")\n",
    "\n",
    "df_bronze = df_bronze.drop(\"data\").drop(\"context\")\n",
    "\n",
    "# -------------------------\n",
    "# Dictionary of DataFrames and their target Silver table names\n",
    "# -------------------------\n",
    "silver_dfs = {\n",
    "    silver_table: df_bronze,             # main events Silver table\n",
    "    f\"{target_cfg['catalog']}.{target_cfg['schema']}.events_data\": df_data,    # data table\n",
    "    f\"{target_cfg['catalog']}.{target_cfg['schema']}.events_context\": df_context  # context table\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# Write each DataFrame to Delta if table doesn't exist\n",
    "# -------------------------\n",
    "for table_name, df in silver_dfs.items():\n",
    "    if not spark.catalog.tableExists(table_name):\n",
    "        print(f\"Creating Silver table {table_name}\")\n",
    "        df.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .saveAsTable(table_name)\n",
    "    else:\n",
    "        print(f\"Merging new data into {table_name}\")\n",
    "        delta_table = DeltaTable.forName(spark, table_name)\n",
    "        merge_condition = \"t.event_id = s.event_id\"\n",
    "        delta_table.alias(\"t\").merge(\n",
    "            df.alias(\"s\"),\n",
    "            merge_condition\n",
    "        ).whenMatchedUpdateAll() \\\n",
    "         .whenNotMatchedInsertAll() \\\n",
    "         .execute()\n",
    "\n",
    "\n",
    "print(\"✅ Merge completed: new events added\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "events_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
